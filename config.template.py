GITHUB_APP_TOKEN = ""

### Model settings ###
# The maximum sequence length that this model might ever be used with
n_positions = n_ctx = 512

# Dimensionality of the embeddings and hidden states.
n_embd = 768

# Number of hidden layers in the Transformer encoder.
n_layer = 12

# Number of attention heads for each attention layer in the Transformer encoder.
n_head = 12